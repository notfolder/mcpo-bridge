# MCPO On-Demand MCP Bridge 詳細設計書

## 1. システム概要

### 1.1 目的

本システムは、OpenWebUI + MCPO 環境において、PowerPoint等の「ファイル生成系MCPサーバー」を数百人規模で安全に利用可能にすることを目的とする。

以下の要件を満たす：
- MCP/MCPOの同期モデルを維持
- マルチユーザー対応
- リソース分離の徹底
- 一時ファイルの確実な削除

### 1.2 背景と課題

既存のMCPサーバーは常駐プロセスを前提とした設計となっており、ファイル生成用途では以下の問題が発生する：

- メモリ常駐によるリソース消費
- 一時ファイルの肥大化
- ユーザー間での成果物混線リスク
- MCP仕様上、非同期ジョブやキューモデルが表現できない

本設計では、「1リクエスト = 1プロセス」という Ephemeral（短命）なプロセスモデルを採用することで、これらの課題を解決する。

### 1.3 設計の位置づけ

本設計は以下の特徴を持つ：

- MCP/MCPO仕様を逸脱しない正統な拡張
- 非同期・キューを導入せず、同期モデルを維持
- 「MCPサーバー = ジョブワーカー」という新しい解釈
- PowerPoint生成のような重いツール用途に特化した現実解

## 2. 要求仕様

### 2.1 機能要件

| ID | 要件 | 説明 |
|---|---|---|
| FR-1 | MCP/MCPO互換リクエスト受信 | MCPおよびMCPO仕様に準拠したリクエストを受信し処理する |
| FR-2 | リクエスト毎プロセス起動 | 各リクエストに対して独立したMCPサーバープロセスを起動する |
| FR-3 | 単一リクエスト処理後終了 | MCPサーバーは1つのリクエストを処理した後、即座に終了する |
| FR-4 | ファイルダウンロード機能 | 生成されたファイルをHTTPS経由でダウンロード可能にする |
| FR-5 | 自動ファイル削除 | 不要となったファイルを自動的に削除する |
| FR-6 | 複数MCPサーバー種類対応 | 異なる種類のMCPサーバーを同時にサポートする |

### 2.2 非機能要件

| ID | 要件 | 説明 |
|---|---|---|
| NFR-1 | 高同時実行性 | 数百の同時リクエストに耐える処理能力を持つ |
| NFR-2 | ユーザー分離 | ユーザー間で成果物が完全に分離される |
| NFR-3 | プロトコル準拠 | MCP/MCPOプロトコル仕様に完全準拠する |
| NFR-4 | ステートレス設計 | 実装はステートレスを基本とし、水平スケール可能とする |
| NFR-5 | 水平スケール対応 | Docker Compose replicasとnginxによるロードバランシング |

## 3. アーキテクチャ設計

### 3.1 全体構成

システムは以下のコンポーネントから構成される：

#### 論理構成図

```
User Browser
   |
   | HTTPS
   v
Nginx (Load Balancer & File Server)
   |
   +-- Load Balancing --> MCPO Bridge Instance 1
   |                      MCPO Bridge Instance 2
   |                      MCPO Bridge Instance N
   |
   +-- Static Files ----> Bind Mount (./data/mcpo-jobs)
   
   
OpenWebUI (Docker Container)
   |
   | MCP / MCPO (JSON-RPC over HTTP)
   v
Nginx Load Balancer
   |
   v
MCPO On-Demand Bridge (Docker Container x N)
   |
   | per-request subprocess
   |   - 作業ディレクトリ作成 (./data/mcpo-jobs/job-uuid)
   |   - MCP Server プロセス起動
   v
Ephemeral MCP Server Process
   |
   | ファイル生成 (pptx, pdf, etc.)
   v
Temporary File Store (Bind Mount)
   |
   | HTTPS download via Nginx
   v
User Browser
```

#### Docker構成

- Nginxコンテナ：フロントエンドプロキシ、ロードバランサー、静的ファイル配信
- OpenWebUIコンテナ：ユーザーインターフェース（Volumeマウント）
- MCPO Bridgeコンテナ：複数インスタンス（replicasで制御、バインドマウント使用）
- すべてのコンテナが同一Docker Composeで管理される
- コンテナ間通信はDockerネットワーク経由
- 一時ファイルとログはローカルディレクトリにバインドマウント

### 3.2 デプロイメント構成

#### 開発環境
- Docker Compose による単一ホスト構成
- Nginx、OpenWebUI、MCPO Bridge（複数レプリカ）が同一ネットワーク上で動作
- バインドマウントによる一時ファイル管理

#### 本番環境（想定）
- 同様のDocker Compose構成
- 共有ストレージ（NFS、S3等）による一時ファイル管理も選択可能
- 外部ロードバランサーとの併用も可能

## 4. コンポーネント詳細設計

### 4.1 Nginx フロントエンド

#### 4.1.1 役割と責務

Nginxコンポーネントは以下の責務を持つ：

- MCPOエンドポイントへのロードバランシング
- 静的ファイル配信（生成されたファイルのダウンロード、有効期限チェックなし）
- リバースプロキシ機能
- アクセスログ記録
- HTTPSターミネーション（本番環境）

#### 4.1.2 エンドポイント設計

##### MCP/MCPOプロキシエンドポイント

- MCPOエンドポイント：/mcpo/{server-type}
  - 例：/mcpo/powerpoint
  - 処理：対応するMCPサーバータイプのBridgeインスタンスへプロキシ
  
- MCPエンドポイント：/mcp/{server-type}
  - 例：/mcp/powerpoint
  - 処理：MCPプロトコルでの通信をプロキシ

##### ファイルダウンロードエンドポイント

- パス：/files/{job-uuid}/{filename}
- メソッド：GET
- 処理：バインドマウントから直接ファイルを配信（シンプルな配信、有効期限チェックなし）
- キャッシュ：無効化（Cache-Control: no-cache）

#### 4.1.3 ロードバランシング設定

- アルゴリズム：ラウンドロビン
- ヘルスチェック：/health エンドポイントを定期的にチェック
- フェイルオーバー：unhealthyなインスタンスを自動除外
- セッションアフィニティ：不要（ステートレス設計）

#### 4.1.4 静的ファイル配信設定

- ドキュメントルート：バインドマウント ./data/mcpo-jobs
- ディレクトリリスティング：無効
- 有効期限チェック：なし（シンプルな配信）
- MIMEタイプ：自動判定

### 4.2 MCPO On-Demand Bridge

#### 4.2.1 役割と責務

Bridgeコンポーネントは以下の責務を持つ：

- MCP/MCPOエンドポイントの提供
- リクエスト受信（パススルー）
- ジョブIDの発行と管理
- 作業ディレクトリの作成
- MCPサーバープロセスの起動と監視
- プロセスとの標準入出力通信
- 処理完了まで同期ブロック
- MCPレスポンスをそのまま返却
- メタデータ更新

#### 4.2.2 技術スタック

- 実装言語：Python 3.11以上
- Webフレームワーク：FastAPI
- 非同期処理：asyncio
- プロセス管理：subprocess モジュール
- HTTPサーバー：Uvicorn
- メトリクス：Prometheus client library
- Docker基盤：Python公式イメージ

#### 4.2.3 主要モジュール構成

システムは以下のモジュールで構成される：

##### エンドポイントモジュール

- MCP/MCPOプロトコルエンドポイントの実装
- サーバータイプごとのエンドポイント提供
- リクエストのパススルー処理
- レスポンスの透過的返却

##### ジョブ管理モジュール

- UUID v4 によるジョブID発行
- ジョブメタデータの管理（作成時刻、状態）
- ジョブディレクトリの作成
- ジョブ状態の追跡

##### プロセス実行モジュール

- MCPサーバープロセスの起動
- 標準入出力パイプの管理
- プロセス監視とタイムアウト処理
- プロセス終了待機と同期ブロック
- 標準出力からのJSON解析
- エラーハンドリングと例外処理

##### ガーベジコレクションモジュール

- 定期的な古いファイル検査
- ディレクトリ削除処理
- 起動時の孤児ディレクトリクリーンアップ
- 安全な削除処理（パス検証）

##### 設定管理モジュール

- MCPサーバー設定ファイル読み込み
- JSON形式設定パース
- 環境変数からの設定オーバーライド
- 設定バリデーション

#### 4.2.4 MCPサーバー設定仕様

BridgeはClaude等で使用されるMCPサーバー設定JSON形式を使用する。

##### 設定ファイル構造

設定ファイルは以下の構造を持つ：

- ルートオブジェクトに「mcpServers」キーが存在
- 各サーバー定義はサーバー名をキーとするオブジェクト
- サーバー定義には「command」と「args」が必須
- 「env」キーでサーバー固有の環境変数を指定可能

##### サーバー定義の要素

各MCPサーバー定義には以下が含まれる：

- command：実行するコマンドパス（絶対パスまたは相対パス）
- args：コマンドライン引数の配列
- env：環境変数のキーバリューマップ（オプショナル）

##### 設定ファイルの配置

- コンテナ内パス：/app/config/mcp-servers.json
- ソースコード配下のconfigディレクトリに配置
- サンプル設定：config/mcp-servers.json.example（Office-PowerPoint-MCP-Serverを使用）
- 設定変更時はコンテナ再起動が必要

#### 4.2.5 並行実行制御

同時実行制御のため以下を実装：

- asyncio.Semaphore による同時起動プロセス数制限
- デフォルト上限：CPU コア数 × 4
- 環境変数による上限カスタマイズ可能
- 上限到達時は429 Too Many Requests を返却

#### 4.2.6 タイムアウト設定

プロセス実行のタイムアウト管理：

- デフォルトタイムアウト：300秒（5分）
- 環境変数による調整可能
- タイムアウト時はプロセス強制終了（SIGTERM → SIGKILL）
- クライアントには504 Gateway Timeout を返却

### 4.3 Ephemeral MCP Server

#### 4.3.1 特性と要件

Ephemeral（短命）MCPサーバーは以下の特性を持つ：

- 1リクエスト = 1プロセスの原則
- 起動時に作業ディレクトリを引数または環境変数で受け取る
- 標準入力からMCPリクエストJSONを読み取る
- 標準出力にMCPレスポンスJSONを出力
- 処理完了後は即座にプロセス終了（exit code 0）
- エラー時は標準エラー出力にメッセージ出力後、非ゼロで終了

#### 4.3.2 実装条件

MCPサーバー実装は以下を満たす必要がある：

- MCP標準仕様に準拠したJSON-RPC通信
- ステートレスな処理（前回実行結果に依存しない）
- 適切なエラーハンドリングと終了コード
- タイムアウト内での処理完了

#### 4.3.3 対応MCPサーバー例

以下のようなMCPサーバーが利用可能：

- Office-PowerPoint-MCP-Server（GongRzhe/Office-PowerPoint-MCP-Server）
  - uvxコマンドで起動：uvx office-powerpoint-mcp-server
  - Dockerfileでuvインストールが必要
- PDF生成サーバー
- Excel生成サーバー
- 画像生成サーバー
- その他ファイル生成系MCPサーバー

### 4.4 一時ファイル管理

#### 4.4.1 ディレクトリ構造

一時ファイルは以下の構造で管理される：

```
./data/mcpo-jobs/
  └── {job-uuid}/
       ├── request.json      # 受信したMCPリクエスト
       ├── response.json     # MCPサーバーからのレスポンス
       ├── metadata.json     # ジョブメタデータ
       ├── output.pptx       # 生成ファイル（例）
       └── server.log        # MCPサーバーログ（オプション）
```

#### 4.4.2 ディレクトリライフサイクル

各ジョブディレクトリは以下のライフサイクルを持つ：

- 作成：リクエスト受信時に作成
- 利用：MCPサーバープロセスによるファイル生成
- 保持：処理完了後も保持
- 削除：ガーベジコレクタにより定期的に削除

#### 4.4.3 メタデータ管理

metadata.jsonには以下の情報を記録：

- job_id：ジョブの一意識別子
- server_name：使用したMCPサーバー名
- created_at：作成日時（ISO 8601形式）
- status：ジョブ状態（processing, completed, failed）
- request：受信したMCPリクエスト
- response：MCPサーバーレスポンス（処理完了後）
- error：エラーメッセージ（失敗時のみ）

#### 4.4.4 セキュリティ考慮

ファイルセキュリティのため以下を実施：

- job-uuidは推測困難なUUID v4を使用
- シンボリックリンク攻撃対策（パス正規化チェック）

#### 4.4.5 ストレージ設計

Docker環境では以下のストレージ設計を採用：

- バインドマウント：./data/mcpo-jobs
- コンテナ内マウントポイント：/tmp/mcpo-jobs
- 全Bridgeインスタンスで共有
- .gitignoreで除外

## 5. 処理フロー設計

### 5.1 正常系処理シーケンス

#### 全体フロー

1. OpenWebUIからMCP/MCPOリクエスト送信
2. NginxがリクエストをBridgeインスタンスにロードバランス
3. Bridge でリクエスト受信（パススルー）
4. UUID によるジョブID発行
5. ジョブディレクトリ作成（./data/mcpo-jobs/{job-uuid}/）
6. メタデータファイル作成
7. リクエストJSONをファイル保存
8. MCPサーバー設定から対象サーバー選択
9. コマンドライン引数組み立て
10. MCPサーバープロセス起動
11. 標準入力にリクエスト送信
12. 標準出力からレスポンス読み取り（同期ブロック）
13. プロセス終了待機
14. メタデータ更新（status=completed）
15. MCPレスポンスをそのまま返却

#### リクエスト処理の詳細フロー

##### Phase 1: リクエスト受付

- HTTPリクエストボディからJSON抽出
- JSON-RPCフォーマット検証（基本的なもののみ）
- パスパラメータからMCPサーバータイプ抽出
- 対応MCPサーバーの存在確認

##### Phase 2: ジョブ準備

- UUID v4生成
- 現在時刻取得
- ジョブディレクトリ作成
- メタデータJSON作成

##### Phase 3: プロセス実行

- MCPサーバー設定読み込み
- コマンド構築
- 環境変数準備
- subprocess.Popen でプロセス起動
- 標準入力へリクエストJSON書き込み
- 標準出力からレスポンスJSON読み取り
- タイムアウト監視
- プロセス終了コード確認

##### Phase 4: レスポンス処理

- メタデータ更新
- MCPレスポンスをそのまま返却（変更なし）
- HTTPレスポンス返却

### 5.2 異常系処理フロー

#### タイムアウト発生時

1. タイムアウト検出
2. MCPサーバープロセスにSIGTERM送信
3. 10秒待機
4. まだ終了していなければSIGKILL送信
5. エラーログ記録
6. ジョブステータスを「failed」に更新
7. クライアントに504エラー返却

#### MCPサーバー異常終了時

1. 非ゼロ終了コード検出
2. 標準エラー出力取得
3. エラーログ記録
4. ジョブステータスを「failed」に更新
5. エラーメッセージをそのまま返却

#### MCPサーバーがエラーを返さない場合

1. MCPサーバーが正常終了（exit code 0）
2. ファイルが存在しなくてもエラーとしない
3. MCPレスポンスをそのまま返却
4. ジョブステータスを「completed」に更新

#### 並行実行制限到達時

1. Semaphore 取得試行
2. 即座に取得できない場合
3. リトライせず429エラー返却
4. Retry-Afterヘッダー付与
5. クライアント側でリトライ推奨

#### ディスク容量不足時

1. ファイル書き込み時にOSError検出
2. エラーログ記録
3. 部分的に作成されたファイル削除
4. ジョブディレクトリ削除
5. クライアントに507エラー返却

### 5.3 ファイルダウンロードフロー（Nginx経由）

#### ダウンロードリクエスト処理（Nginx）

1. ダウンロードURLにアクセス（GET /files/{job-uuid}/{filename}）
2. job-uuid抽出
3. ファイルパス構築
4. ファイル存在確認
5. MIME type判定
6. Content-Dispositionヘッダー設定
7. ファイルストリーミング送信

## 6. API仕様設計

### 6.1 エンドポイント一覧

#### 6.1.1 MCPOエンドポイント（複数サーバータイプ対応）

各MCPサーバータイプごとに独立したエンドポイントを提供：

- パス：/mcpo/{server-type}
- 例：
  - /mcpo/powerpoint
  - /mcpo/pdf-generator
- メソッド：POST
- Content-Type：application/json
- プロトコル：JSON-RPC 2.0

#### 6.1.2 MCPエンドポイント（複数サーバータイプ対応）

各MCPサーバータイプごとに独立したエンドポイントを提供：

- パス：/mcp/{server-type}
- 例：
  - /mcp/powerpoint
  - /mcp/pdf-generator  
- メソッド：POST
- Content-Type：application/json
- プロトコル：MCP標準プロトコル

#### 6.1.3 ヘルスチェックエンドポイント

- パス：/health
- メソッド：GET
- レスポンス：JSON

### 6.2 MCPOエンドポイント詳細

#### リクエスト形式

JSON-RPC 2.0形式のリクエストを受け付け、そのままMCPサーバーに転送：

- jsonrpc：バージョン文字列（固定値「2.0」）
- method：呼び出すツール名（MCPサーバーで定義されたツール）
- params：ツールパラメータオブジェクト（バリデーションなし）
- id：リクエストID（数値または文字列）

#### レスポンス形式

MCPサーバーからのレスポンスをそのまま返却：

- Bridge側でレスポンスの内容を変更しない
- MCPサーバーが生成したJSONをそのままクライアントに転送
- エラーレスポンスもMCPサーバーの形式をそのまま使用

### 6.3 MCPエンドポイント詳細

#### リクエスト形式

MCP標準プロトコルのリクエストを受け付け、そのままMCPサーバーに転送：

- MCP仕様に従ったリクエスト形式
- Bridgeではバリデーションを行わず、MCPサーバーに委譲

#### レスポンス形式

MCPサーバーからのレスポンスをそのまま返却：

- Bridge側でレスポンスの内容を変更しない
- MCPサーバーが生成したレスポンスをそのままクライアントに転送

### 6.4 ヘルスチェックエンドポイント

#### エンドポイント仕様

- メソッド：GET
- パス：/health
- レスポンス：JSON

#### レスポンス内容

- status：健全性ステータス（ok / degraded / down）
- timestamp：チェック実行時刻
- version：Bridgeバージョン
- uptime：稼働時間（秒）

#### 判定基準

- 正常：すべてのコンポーネントが動作中
- 劣化：一部リソースが制限に近い
- 停止：重大な問題発生



## 7. 環境変数設計

### 7.1 Bridge設定用環境変数

- MCPO_CONFIG_FILE：MCP設定ファイルパス（デフォルト：/app/config/mcp-servers.json）
- MCPO_JOBS_DIR：ジョブディレクトリルート（デフォルト：/tmp/mcpo-jobs）
- MCPO_BASE_URL：ダウンロードURL用ベースURL（デフォルト：http://nginx）
- MCPO_MAX_CONCURRENT：最大同時実行数（デフォルト：CPU数×4）
- MCPO_TIMEOUT：プロセスタイムアウト（秒）（デフォルト：300）
- MCPO_LOG_LEVEL：ログレベル（デフォルト：INFO）

### 7.2 MCPサーバー実行時環境変数

MCPサーバープロセスに渡される環境変数：

- MCPO_WORKDIR：作業ディレクトリパス
- MCPO_JOB_ID：ジョブID
- その他、mcp-servers.jsonのenv設定

## 8. セキュリティ設計

### 8.1 基本方針

本システムでは、以下のセキュリティ方針を採用：

- 入力検証はすべてMCPサーバーに委譲
- Dockerコンテナはroot権限で実行
- シンプルな設計を優先

### 8.2 実施するセキュリティ対策

#### パス検証

- ジョブディレクトリパス正規化
- 親ディレクトリトラバーサル防止
- シンボリックリンク検出
- ホワイトリストベースパス検証

#### ファイル名検証

- 許可文字制限（英数字、ハイフン、アンダースコア、ドット）
- 長さ制限（255バイト以内）

#### Docker セキュリティ

##### イメージセキュリティ

- 公式Pythonイメージをベースに使用
- 最小限のパッケージインストール
- 定期的なイメージ更新
- 脆弱性スキャン実施

##### ネットワークセキュリティ

- 内部ネットワーク分離
- 不要ポート非公開
- TLS通信（本番環境）

## 9. スケーラビリティ設計

### 9.1 垂直スケール（単一インスタンス）

#### CPU拡張

- マルチコアCPUによる並列処理
- asyncioによる非同期I/O活用
- Semaphoreによる並行数最適化

#### メモリ拡張

- ファイルストリーミング送信によるメモリ節約
- ジョブメタデータの軽量化
- 不要オブジェクトの早期解放

#### ストレージ拡張

- ディレクトリサイズ拡張
- 古いファイルの積極的削除
- ストレージ使用量監視

### 9.2 水平スケール（複数インスタンス）

#### Docker Compose Replicas

- docker-compose.ymlでreplicas設定
- 同一設定の複数Bridgeインスタンス起動
- 自動的にNginxロードバランサー配下に配置

#### ステートレス設計

- インスタンス間で状態共有なし
- セッション情報なし
- ジョブIDによる一意性担保

#### Nginxロードバランシング

- ラウンドロビン分散
- ヘルスチェック連動
- セッションアフィニティ不要
- フェイルオーバー自動処理

#### 共有ストレージ

- バインドマウントで全インスタンス共有
- 全インスタンスが同一ストレージアクセス
- ダウンロードリクエストはどのインスタンスでも処理可能（Nginx経由）

### 9.3 性能目標

#### レスポンスタイム

- MCPリクエスト受信からレスポンス返却まで：平均5秒以内
- ファイルダウンロード開始まで：1秒以内

#### スループット

- 単一インスタンス：10リクエスト/秒
- 複数インスタンス：100リクエスト/秒以上

#### 同時接続数

- 単一インスタンス：50並行処理
- 複数インスタンス：500並行処理以上

## 10. ガーベジコレクション設計

### 10.1 削除対象の判定

#### 古いジョブ

- 作成日時から一定時間経過したジョブ（デフォルト24時間）
- metadata.jsonのcreated_at参照

#### 孤児ディレクトリ

- metadata.jsonが存在しないディレクトリ
- 作成から一定時間経過（デフォルト24時間）
- 異常終了等で残存したディレクトリ

### 10.2 削除タイミング

#### 定期実行（cron方式）

- 実行間隔：1時間毎（デフォルト）
- バックグラウンドタスクとして実行
- asyncioスケジューラー利用

#### 起動時クリーンアップ

- Bridge起動時に1回実行
- 前回異常終了時の残存ファイル削除
- 整合性確保

#### cron設定例

外部cronジョブでガーベジコレクションを実行する場合の設定例：

```bash
# 毎時0分にガーベジコレクションを実行
0 * * * * docker exec mcpo-bridge python -m src.utils.gc_jobs

# 毎日午前3時にガーベジコレクションを実行
0 3 * * * docker exec mcpo-bridge python -m src.utils.gc_jobs
```

または、ホストマシンから直接ディレクトリをクリーンアップする場合：

```bash
# 24時間以上経過したジョブディレクトリを削除
0 * * * * find /path/to/data/mcpo-jobs -mindepth 1 -maxdepth 1 -type d -mtime +1 -exec rm -rf {} \;
```

### 10.3 削除処理フロー

#### Phase 1: スキャン

1. ./data/mcpo-jobs ディレクトリ走査
2. 各ジョブディレクトリのmetadata.json読み込み
3. created_at確認
4. 削除対象リスト作成

#### Phase 2: 削除実行

1. 削除対象ディレクトリに対して順次処理
2. ディレクトリ内全ファイル削除
3. ディレクトリ自体を削除
4. 削除成功ログ記録

#### Phase 3: エラーハンドリング

1. 削除失敗時はログ記録
2. 次回GC実行時にリトライ

### 10.4 安全性担保

#### パス検証

- 削除対象が./data/mcpo-jobs配下か厳密確認
- 親ディレクトリ削除防止
- シンボリックリンク追跡禁止

#### ログ記録

- 削除ジョブID記録
- 削除日時記録
- エラー詳細記録

## 11. エラーハンドリング設計

### 11.1 エラー分類

#### レベル1：リクエストエラー

- クライアント起因のエラー
- 4xxステータスコード
- リトライ不要

#### レベル2：処理エラー

- MCPサーバー起因のエラー
- 5xxステータスコード
- リトライ可能な場合あり

#### レベル3：システムエラー

- Bridge自体の障害
- 503ステータスコード
- リトライ推奨

### 11.2 エラーレスポンス設計

#### 基本方針

MCPサーバーからのエラーレスポンスをそのまま返却：

- Bridgeでエラーメッセージを加工しない
- MCPサーバーのエラー形式を保持
- トレース情報もMCPサーバーの出力をそのまま使用

### 11.3 ログ設計

#### ログレベル

- DEBUG：デバッグ情報
- INFO：通常動作ログ
- WARNING：警告（処理は継続）
- ERROR：エラー（処理失敗）
- CRITICAL：致命的エラー（サービス停止）

#### ログ出力先

- 標準出力（Dockerログに集約）
- ログディレクトリに永続化（./data/mcpo-logs、バインドマウント）
- 本番環境では外部ログ集約サービス連携

#### ログフォーマット

- JSON形式
- タイムスタンプ、ログレベル、メッセージ、コンテキスト情報含む
- トレースIDによるリクエスト追跡



## 12. Docker化設計

### 12.1 Dockerfile設計方針

#### ベースイメージ

- 公式Pythonイメージ使用（python:3.11）
- セキュリティアップデート適用済みイメージ選定
- root権限での実行
- シンプルな単一ステージビルド

#### レイヤー最適化

- 変更頻度の低いコマンドを先に配置
- キャッシュ効率最大化
- 不要ファイル除外

### 12.2 ディレクトリ構造

コンテナ内ディレクトリ配置：

```
/app/
  ├── main.py                 # アプリケーションエントリーポイント
  ├── requirements.txt        # Python依存パッケージ
  ├── config/                 # 設定ファイルディレクトリ（ソースコード配下）
  │   ├── mcp-servers.json    # MCPサーバー定義
  │   └── mcp-servers.json.example  # サンプル設定
  └── src/                    # ソースコードディレクトリ
      ├── api/                # APIエンドポイント
      ├── core/               # コアロジック
      ├── models/             # データモデル
      └── utils/              # ユーティリティ

/tmp/mcpo-jobs/             # 一時ファイル作業領域（バインドマウント）

/var/log/mcpo/              # ログディレクトリ（バインドマウント）
```

### 12.3 依存パッケージ管理

#### Python依存パッケージ

requirements.txtで管理する主要パッケージ：

- FastAPI：Webフレームワーク
- Uvicorn：ASGIサーバー
- Pydantic：データバリデーション
- aiofiles：非同期ファイルI/O
- python-multipart：ファイルアップロード対応

#### システムパッケージ

実行時に必要なパッケージ：

- curl：ヘルスチェック用
- ca-certificates：HTTPS通信用

### 12.4 ストレージ設計

#### 一時ファイルストレージ

- バインドマウント：./data/mcpo-jobs
- マウントポイント：/tmp/mcpo-jobs
- 全Bridgeインスタンスで共有
- .gitignoreで除外

#### ログストレージ

- バインドマウント：./data/mcpo-logs
- マウントポイント：/var/log/mcpo
- 永続化ログ保存
- 全Bridgeインスタンスで共有
- .gitignoreで除外

### 12.5 ネットワーク設計

#### 内部ネットワーク

- ネットワーク名：mcpo-network
- ドライバー：bridge
- Nginx、OpenWebUI、Bridgeが接続

#### ポート公開

- Nginxポート：80（HTTP）、443（HTTPS、本番環境）
- Bridgeポート：8080（内部のみ、Nginx経由）
- OpenWebUIポート：内部のみ（Nginx経由）

### 12.6 環境変数設計

#### 必須環境変数

- MCPO_BASE_URL：ダウンロードURL生成用
- 例：http://nginx

#### オプション環境変数

- MCPO_MAX_CONCURRENT：最大同時実行数
- MCPO_TIMEOUT：タイムアウト時間
- MCPO_LOG_LEVEL：ログレベル

### 12.7 ヘルスチェック設計

#### Dockerヘルスチェック

- チェック方法：HTTP GET /health
- 間隔：30秒
- タイムアウト：10秒
- リトライ：3回
- 開始遅延：10秒

#### ヘルスチェック判定

- 成功：200 OK応答
- 失敗：タイムアウトまたは非200応答
- unhealthy判定後の動作：Docker依存

## 13. Docker Compose設計

### 13.1 サービス定義

#### Nginxサービス

- サービス名：nginx
- イメージ：公式nginxイメージ
- ポート：ホスト80 → コンテナ80
- バインドマウント：nginx設定、./data/mcpo-jobs（読み取り専用）
- ネットワーク：mcpo-network
- 設定：ロードバランシング、静的ファイル配信

#### OpenWebUIサービス

- サービス名：openwebui
- イメージ：公式OpenWebUIイメージ
- ポート：内部のみ（Nginx経由）
- ボリューム：openwebui-data（永続化）
- ネットワーク：mcpo-network
- 依存関係：mcpo-bridge（depends_on）

#### MCPO Bridgeサービス

- サービス名：mcpo-bridge
- ビルド：ローカルDockerfile使用
- ポート：内部8080（Nginx経由のみ）
- バインドマウント：./data/mcpo-jobs、./data/mcpo-logs
- ネットワーク：mcpo-network
- ヘルスチェック：有効
- デプロイ：replicas設定で複数インスタンス

### 13.2 ストレージ定義

#### OpenWebUIデータ（ボリューム）

- ボリューム名：openwebui-data
- 用途：OpenWebUIデータ永続化

#### MCPO Bridge データ（バインドマウント）

- ./data/mcpo-jobs：一時ファイル（.gitignore）
- ./data/mcpo-logs：ログファイル（.gitignore）

### 13.3 ネットワーク定義

#### mcpo-network

- ドライバー：bridge
- 内部通信専用
- DNSによるサービス名解決

### 13.4 スケール設定

#### Replicas設定

- mcpo-bridgeサービスにdeploy.replicas設定
- デフォルト：3インスタンス
- Nginxが自動的にロードバランシング

### 13.5 起動順序制御

#### depends_on設定

- OpenWebUIはmcpo-bridgeに依存
- mcpo-bridgeのhealthy確認後にOpenWebUI起動
- condition: service_healthy 利用

#### 起動シーケンス

1. ネットワーク作成
2. ボリューム作成
3. mcpo-bridge起動（複数レプリカ）
4. mcpo-bridgeヘルスチェック待機
5. nginx起動
6. openwebui起動
7. システム利用可能

### 13.6 再起動ポリシー

#### unless-stoppedポリシー

採用理由：
- 異常終了時の自動復旧
- ホスト再起動時の自動起動
- 手動停止時は再起動しない

## 14. MCP設定ファイル詳細仕様

### 14.1 設定ファイル配置

#### ファイルパス

- ソースコード配下：/app/config/mcp-servers.json
- サンプルファイル：/app/config/mcp-servers.json.example
- 環境変数で変更可能：MCPO_CONFIG_FILE

#### ファイル形式

- エンコーディング：UTF-8
- フォーマット：JSON
- スキーマバリデーション：起動時実施

### 14.2 設定ファイル構造詳細

#### ルートオブジェクト

- mcpServers：サーバー定義マップ（必須）

#### サーバー定義オブジェクト

各サーバーは以下のフィールドを持つ：

- command：実行バイナリパス（必須）
  - 絶対パスまたは相対パス
  - PATH環境変数で解決可能なコマンド名
- args：コマンドライン引数配列（必須、空配列可）
  - 文字列の配列
- env：環境変数マップ（オプション）
  - キーバリューペアのオブジェクト
  - サーバー実行時に設定
- timeout：タイムアウト秒数（オプション）
  - 個別サーバーのタイムアウト設定
  - グローバル設定を上書き

### 14.3 設定例（Office-PowerPoint-MCP-Server）

サンプル設定ではOffice-PowerPoint-MCP-Serverを使用：

- サーバー名：powerpoint
- コマンド：uvx
- 引数：office-powerpoint-mcp-server
- 環境変数：なし（uvxが環境を管理）

注意：Dockerfileでuvコマンドをインストールする必要があります。

### 14.4 設定リロード

#### 現行仕様

- 設定ファイル変更時はBridge再起動が必要
- 動的リロード機能なし

## 15. Nginx設定設計

### 15.1 ロードバランサー設定

#### アップストリーム定義

- mcpo-bridgeサービスの全レプリカを自動検出
- Docker DNSによる名前解決
- ラウンドロビンアルゴリズム

#### ヘルスチェック

- 各Bridgeインスタンスの/healthエンドポイントを監視
- unhealthyなインスタンスを自動除外

### 15.2 静的ファイル配信設定

#### ロケーション設定

- パス：/files/
- ルート：バインドマウント ./data/mcpo-jobs
- ディレクトリリスティング：無効

#### シンプルな配信

- 有効期限チェックなし
- シンプルなファイル配信
- 404エラーは存在しないファイルのみ

### 15.3 プロキシ設定

#### MCPOエンドポイント

- パス：/mcpo/
- プロキシ先：mcpo-bridgeアップストリーム
- タイムアウト：600秒（長時間処理対応）

#### MCPエンドポイント

- パス：/mcp/
- プロキシ先：mcpo-bridgeアップストリーム
- タイムアウト：600秒

## 16. 運用設計

### 16.1 デプロイ手順

#### 初回デプロイ

1. リポジトリクローン
2. config/mcp-servers.json.example を config/mcp-servers.json にコピーして編集
3. docker-compose.yml 環境変数調整
4. docker-compose up -d 実行
5. ヘルスチェック確認
6. OpenWebUI アクセス確認

#### 更新デプロイ

1. 最新コードpull
2. docker-compose build 実行
3. docker-compose up -d 実行
4. ヘルスチェック確認
5. 動作確認

### 16.2 監視項目

#### サービス監視

- コンテナ稼働状態
- ヘルスチェック結果
- リスタート回数

#### リソース監視

- CPU使用率
- メモリ使用量
- ディスク使用量（バインドマウント）
- ネットワークトラフィック

#### アプリケーション監視

- リクエスト数
- エラー率
- レスポンスタイム
- 同時実行プロセス数

### 16.3 障害対応

#### コンテナ再起動

- docker-compose restart mcpo-bridge
- 自動再起動ポリシーにより自動復旧も可能

#### ログ確認

- docker-compose logs mcpo-bridge
- バインドマウントから直接確認（./data/mcpo-logs）
- エラー原因特定

#### ロールバック

- 前バージョンイメージへの切り戻し
- docker-compose down → イメージタグ変更 → up

### 16.4 スケールアウト

#### 手動スケール

- docker-compose.ymlのreplicas設定変更
- docker-compose up -d で反映
- Nginxが自動的に新インスタンスを検出

## 17. テスト設計

### 17.1 ユニットテスト

#### テスト対象

- ジョブ管理関数
- メタデータ生成関数

#### テストフレームワーク

- pytest
- モックライブラリ使用

### 17.2 統合テスト

#### テストシナリオ

- エンドツーエンドMCP/MCPOリクエスト処理
- ファイルダウンロード（Nginx経由）
- エラーハンドリング
- タイムアウト処理

#### テスト環境

- Docker Compose によるテスト環境構築
- モックMCPサーバー使用

### 17.3 負荷テスト

#### テストツール

- Apache Bench (ab)
- Locust
- k6

#### テスト項目

- 同時リクエスト数増加
- レスポンスタイム測定
- エラー率測定
- スケーリング効果確認

## 18. まとめ

本詳細設計書は、MCPO On-Demand MCP Bridgeシステムの実装に必要な全ての設計情報を網羅している。

### 主要な設計ポイント

- Nginxフロントエンドによる効率的なファイル配信とロードバランシング
- MCP/MCPOエンドポイントの明確な分離と複数サーバータイプ対応
- MCPレスポンスの透過的な転送（変更なし）
- 入力検証のMCPサーバーへの委譲
- Docker Compose replicasによる簡単なスケールアウト
- バインドマウントによるシンプルなストレージ管理
- Office-PowerPoint-MCP-Serverをuvxで起動する設定例
- シンプルで保守しやすい設計

本設計に基づき実装を進めることで、安全でスケーラブルなファイル生成系MCPサーバー基盤を構築できる。

---

**MCPO On-Demand Bridge 詳細設計書 v3.0**
