# MCPO On-Demand MCP Bridge 詳細設計書

## 1. システム概要

### 1.1 目的

本システムは、OpenWebUI + MCPO 環境において、PowerPoint等の「ファイル生成系MCPサーバー」を数百人規模で安全に利用可能にすることを目的とする。

以下の要件を満たす：
- MCP/MCPOの同期モデルを維持
- マルチユーザー対応
- リソース分離の徹底
- 一時ファイルの確実な削除

### 1.2 背景と課題

既存のMCPサーバーは常駐プロセスを前提とした設計となっており、ファイル生成用途では以下の問題が発生する：

- メモリ常駐によるリソース消費
- 一時ファイルの肥大化
- ユーザー間での成果物混線リスク
- MCP仕様上、非同期ジョブやキューモデルが表現できない

本設計では、「1リクエスト = 1プロセス」という Ephemeral（短命）なプロセスモデルを採用することで、これらの課題を解決する。

### 1.3 設計の位置づけ

本設計は以下の特徴を持つ：

- MCP/MCPO仕様を逸脱しない正統な拡張
- 非同期・キューを導入せず、同期モデルを維持
- 「MCPサーバー = ジョブワーカー」という新しい解釈
- PowerPoint生成のような重いツール用途に特化した現実解

## 2. 要求仕様

### 2.1 機能要件

| ID | 要件 | 説明 |
|---|---|---|
| FR-1 | MCP/MCPO互換リクエスト受信 | MCPおよびMCPO仕様に準拠したリクエストを受信し処理する |
| FR-2 | リクエスト毎プロセス起動 | 各リクエストに対して独立したMCPサーバープロセスを起動する |
| FR-3 | 単一リクエスト処理後終了 | MCPサーバーは1つのリクエストを処理した後、即座に終了する |
| FR-4 | ファイルダウンロード機能 | 生成されたファイルをHTTPS経由でダウンロード可能にする |
| FR-5 | ダウンロードURL有効期限 | ダウンロードURLに有効期限を設定し、期限後はアクセス不可とする |
| FR-6 | 自動ファイル削除 | 不要となったファイルを自動的に削除する |
| FR-7 | 複数MCPサーバー種類対応 | 異なる種類のMCPサーバーを同時にサポートする |
| FR-8 | メトリクス収集 | Prometheus形式でメトリクスを公開する |

### 2.2 非機能要件

| ID | 要件 | 説明 |
|---|---|---|
| NFR-1 | 高同時実行性 | 数百の同時リクエストに耐える処理能力を持つ |
| NFR-2 | ユーザー分離 | ユーザー間で成果物が完全に分離される |
| NFR-3 | プロトコル準拠 | MCP/MCPOプロトコル仕様に完全準拠する |
| NFR-4 | ステートレス設計 | 実装はステートレスを基本とし、水平スケール可能とする |
| NFR-5 | 水平スケール対応 | Docker Compose replicasとnginxによるロードバランシング |

## 3. アーキテクチャ設計

### 3.1 全体構成

システムは以下のコンポーネントから構成される：

#### 論理構成図

```
User Browser
   |
   | HTTPS
   v
Nginx (Load Balancer & File Server)
   |
   +-- Load Balancing --> MCPO Bridge Instance 1
   |                      MCPO Bridge Instance 2
   |                      MCPO Bridge Instance N
   |
   +-- Static Files ----> Shared Volume (/tmp/mcpo-jobs)
   
   
OpenWebUI (Docker Container)
   |
   | MCP / MCPO (JSON-RPC over HTTP)
   v
Nginx Load Balancer
   |
   v
MCPO On-Demand Bridge (Docker Container x N)
   |
   | per-request subprocess
   |   - 作業ディレクトリ作成 (/tmp/mcpo-jobs/job-uuid)
   |   - MCP Server プロセス起動
   v
Ephemeral MCP Server Process
   |
   | ファイル生成 (pptx, pdf, etc.)
   v
Temporary File Store (Shared Volume)
   |
   | HTTPS download via Nginx
   v
User Browser
```

#### Docker構成

- Nginxコンテナ：フロントエンドプロキシ、ロードバランサー、静的ファイル配信
- OpenWebUIコンテナ：ユーザーインターフェース
- MCPO Bridgeコンテナ：複数インスタンス（replicasで制御）
- すべてのコンテナが同一Docker Composeで管理される
- コンテナ間通信はDockerネットワーク経由
- 一時ファイルは共有Docker Volumeで永続化

### 3.2 デプロイメント構成

#### 開発環境
- Docker Compose による単一ホスト構成
- Nginx、OpenWebUI、MCPO Bridge（複数レプリカ）が同一ネットワーク上で動作
- ローカルボリュームを使用した一時ファイル管理

#### 本番環境（想定）
- 同様のDocker Compose構成
- 共有ストレージ（NFS、S3等）による一時ファイル管理も選択可能
- 外部ロードバランサーとの併用も可能

## 4. コンポーネント詳細設計

### 4.1 Nginx フロントエンド

#### 4.1.1 役割と責務

Nginxコンポーネントは以下の責務を持つ：

- MCPOエンドポイントへのロードバランシング
- 静的ファイル配信（生成されたファイルのダウンロード）
- リバースプロキシ機能
- アクセスログ記録
- HTTPSターミネーション（本番環境）

#### 4.1.2 エンドポイント設計

##### ファイルダウンロードエンドポイント

- パス：/files/{job-uuid}/{filename}
- メソッド：GET
- 処理：共有ボリュームから直接ファイルを配信
- キャッシュ：無効化（Cache-Control: no-cache）
- 有効期限チェック：metadata.jsonを参照

##### MCP/MCPOプロキシエンドポイント

- MCPOエンドポイント：/mcpo/{server-type}
  - 例：/mcpo/pptx-generator、/mcpo/pdf-generator
  - 処理：対応するMCPサーバータイプのBridgeインスタンスへプロキシ
  
- MCPエンドポイント：/mcp/{server-type}
  - 例：/mcp/pptx-generator、/mcp/pdf-generator
  - 処理：MCPプロトコルでの通信をプロキシ

##### メトリクスエンドポイント

- パス：/metrics
- メソッド：GET
- 処理：各Bridgeインスタンスのメトリクスを集約

#### 4.1.3 ロードバランシング設定

- アルゴリズム：ラウンドロビン
- ヘルスチェック：/health エンドポイントを定期的にチェック
- フェイルオーバー：unhealthyなインスタンスを自動除外
- セッションアフィニティ：不要（ステートレス設計）

#### 4.1.4 静的ファイル配信設定

- ドキュメントルート：共有ボリューム /tmp/mcpo-jobs
- ディレクトリリスティング：無効
- 有効期限チェック：metadata.jsonのexpires_atを参照し、期限切れは404返却
- MIMEタイプ：自動判定

### 4.2 MCPO On-Demand Bridge

#### 4.2.1 役割と責務

Bridgeコンポーネントは以下の責務を持つ：

- MCP/MCPOエンドポイントの提供
- リクエスト受信（パススルー）
- ジョブIDの発行と管理
- 作業ディレクトリの作成
- MCPサーバープロセスの起動と監視
- プロセスとの標準入出力通信
- 処理完了まで同期ブロック
- MCPレスポンスをそのまま返却
- ダウンロードURL生成
- メトリクス収集

#### 4.2.2 技術スタック

- 実装言語：Python 3.11以上
- Webフレームワーク：FastAPI
- 非同期処理：asyncio
- プロセス管理：subprocess モジュール
- HTTPサーバー：Uvicorn
- メトリクス：Prometheus client library
- Docker基盤：Python公式イメージ

#### 4.2.3 主要モジュール構成

システムは以下のモジュールで構成される：

##### エンドポイントモジュール

- MCP/MCPOプロトコルエンドポイントの実装
- サーバータイプごとのエンドポイント提供
- リクエストのパススルー処理
- レスポンスの透過的返却

##### ジョブ管理モジュール

- UUID v4 によるジョブID発行
- ジョブメタデータの管理（作成時刻、有効期限、状態）
- ジョブディレクトリの作成
- ジョブ状態の追跡

##### プロセス実行モジュール

- MCPサーバープロセスの起動
- 標準入出力パイプの管理
- プロセス監視とタイムアウト処理
- プロセス終了待機と同期ブロック
- 標準出力からのJSON解析
- エラーハンドリングと例外処理

##### ガーベジコレクションモジュール

- 定期的な期限切れファイル検査
- ディレクトリ削除処理
- 起動時の孤児ディレクトリクリーンアップ
- 安全な削除処理（パス検証）

##### 設定管理モジュール

- MCPサーバー設定ファイル読み込み
- JSON形式設定パース
- 環境変数からの設定オーバーライド
- 設定バリデーション

##### メトリクス収集モジュール

- Prometheusメトリクスの収集と公開
- リクエスト数、処理時間、エラー率の記録
- プロセス起動数、同時実行数の追跡
- ジョブ状態の統計

#### 4.2.4 MCPサーバー設定仕様

BridgeはClaude等で使用されるMCPサーバー設定JSON形式を使用する。

##### 設定ファイル構造

設定ファイルは以下の構造を持つ：

- ルートオブジェクトに「mcpServers」キーが存在
- 各サーバー定義はサーバー名をキーとするオブジェクト
- サーバー定義には「command」と「args」が必須
- 「env」キーでサーバー固有の環境変数を指定可能

##### サーバー定義の要素

各MCPサーバー定義には以下が含まれる：

- command：実行するコマンドパス（絶対パスまたは相対パス）
- args：コマンドライン引数の配列
- env：環境変数のキーバリューマップ（オプショナル）

##### 作業ディレクトリの指定方法

MCPサーバーに作業ディレクトリを渡すには以下の方法を使用：

- args配列内に特殊トークン「__WORKDIR__」を記述
- Bridge実行時に実際のジョブディレクトリパスに置換される
- 環境変数による指定も可能（MCPO_WORKDIR等）

##### 設定ファイルの配置

- コンテナ内パス：/app/config/mcp-servers.json
- ソースコード配下のconfigディレクトリに配置
- サンプル設定：config/mcp-servers.json.example
- 設定変更時はコンテナ再起動が必要

#### 4.2.5 並行実行制御

同時実行制御のため以下を実装：

- asyncio.Semaphore による同時起動プロセス数制限
- デフォルト上限：CPU コア数 × 4
- 環境変数による上限カスタマイズ可能
- 上限到達時は429 Too Many Requests を返却

#### 4.2.6 タイムアウト設定

プロセス実行のタイムアウト管理：

- デフォルトタイムアウト：300秒（5分）
- 環境変数による調整可能
- タイムアウト時はプロセス強制終了（SIGTERM → SIGKILL）
- クライアントには504 Gateway Timeout を返却

### 4.3 Ephemeral MCP Server

#### 4.3.1 特性と要件

Ephemeral（短命）MCPサーバーは以下の特性を持つ：

- 1リクエスト = 1プロセスの原則
- 起動時に作業ディレクトリを引数または環境変数で受け取る
- 標準入力からMCPリクエストJSONを読み取る
- 標準出力にMCPレスポンスJSONを出力
- 処理完了後は即座にプロセス終了（exit code 0）
- エラー時は標準エラー出力にメッセージ出力後、非ゼロで終了

#### 4.3.2 実装条件

MCPサーバー実装は以下を満たす必要がある：

- MCP標準仕様に準拠したJSON-RPC通信
- ファイル出力先を引数で指定可能な設計
- ステートレスな処理（前回実行結果に依存しない）
- 適切なエラーハンドリングと終了コード
- タイムアウト内での処理完了

#### 4.3.3 対応MCPサーバー例

以下のようなMCPサーバーが利用可能：

- PowerPoint生成サーバー（pptx-mcp-server 等）
- PDF生成サーバー
- Excel生成サーバー
- 画像生成サーバー
- その他ファイル生成系MCPサーバー

### 4.4 一時ファイル管理

#### 4.4.1 ディレクトリ構造

一時ファイルは以下の構造で管理される：

```
/tmp/mcpo-jobs/
  └── {job-uuid}/
       ├── request.json      # 受信したMCPリクエスト
       ├── response.json     # MCPサーバーからのレスポンス
       ├── metadata.json     # ジョブメタデータ
       ├── output.pptx       # 生成ファイル（例）
       └── server.log        # MCPサーバーログ（オプション）
```

#### 4.4.2 ディレクトリライフサイクル

各ジョブディレクトリは以下のライフサイクルを持つ：

- 作成：リクエスト受信時に作成
- 利用：MCPサーバープロセスによるファイル生成
- 保持：処理完了後も一定期間保持（デフォルト1時間）
- 削除：有効期限切れ後、ガーベジコレクタにより削除

#### 4.4.3 メタデータ管理

metadata.jsonには以下の情報を記録：

- job_id：ジョブの一意識別子
- server_name：使用したMCPサーバー名
- created_at：作成日時（ISO 8601形式）
- expires_at：有効期限（ISO 8601形式）
- status：ジョブ状態（processing, completed, failed）
- request：受信したMCPリクエスト
- response：MCPサーバーレスポンス（処理完了後）
- error：エラーメッセージ（失敗時のみ）
- output_files：生成ファイル情報配列
  - filename：ファイル名
  - size：サイズ（バイト）
  - mime_type：MIMEタイプ

#### 4.4.4 セキュリティ考慮

ファイルセキュリティのため以下を実施：

- job-uuidは推測困難なUUID v4を使用
- シンボリックリンク攻撃対策（パス正規化チェック）

#### 4.4.5 Docker Volume設計

Docker環境では以下のVolume設計を採用：

- 名前付きVolume「mcpo-jobs」を使用
- コンテナ内マウントポイント：/tmp/mcpo-jobs
- 全Bridgeインスタンスで共有
- ホスト側パスは Docker が自動管理
- 開発環境ではバインドマウントも選択可能

## 5. 処理フロー設計

### 5.1 正常系処理シーケンス

#### 全体フロー

1. OpenWebUIからMCP/MCPOリクエスト送信
2. NginxがリクエストをBridgeインスタンスにロードバランス
3. Bridge でリクエスト受信（パススルー）
4. UUID によるジョブID発行
5. ジョブディレクトリ作成（/tmp/mcpo-jobs/{job-uuid}/）
6. メタデータファイル作成
7. リクエストJSONをファイル保存
8. MCPサーバー設定から対象サーバー選択
9. コマンドライン引数組み立て（__WORKDIR__置換）
10. MCPサーバープロセス起動
11. 標準入力にリクエスト送信
12. 標準出力からレスポンス読み取り（同期ブロック）
13. プロセス終了待機
14. 生成ファイル確認
15. ダウンロードURL生成（Nginx経由のURL）
16. メタデータ更新（status=completed）
17. MCPレスポンスをそのまま返却
18. メトリクス更新

#### リクエスト処理の詳細フロー

##### Phase 1: リクエスト受付

- HTTPリクエストボディからJSON抽出
- JSON-RPCフォーマット検証（基本的なもののみ）
- パスパラメータからMCPサーバータイプ抽出
- 対応MCPサーバーの存在確認

##### Phase 2: ジョブ準備

- UUID v4生成
- 現在時刻取得
- 有効期限計算（現在時刻 + 設定値）
- ジョブディレクトリ作成
- メタデータJSON作成

##### Phase 3: プロセス実行

- MCPサーバー設定読み込み
- コマンド構築（パス置換処理）
- 環境変数準備
- subprocess.Popen でプロセス起動
- 標準入力へリクエストJSON書き込み
- 標準出力からレスポンスJSON読み取り
- タイムアウト監視
- プロセス終了コード確認

##### Phase 4: レスポンス処理

- 生成ファイル検索（ディレクトリスキャン）
- ファイル情報取得（サイズ、MIME type）
- ダウンロードURL組み立て（Nginx経由）
- MCPレスポンスをそのまま返却（変更なし）
- メタデータ更新
- HTTPレスポンス返却

### 5.2 異常系処理フロー

#### タイムアウト発生時

1. タイムアウト検出
2. MCPサーバープロセスにSIGTERM送信
3. 10秒待機
4. まだ終了していなければSIGKILL送信
5. エラーログ記録
6. ジョブステータスを「failed」に更新
7. クライアントに504エラー返却
8. メトリクス更新（エラーカウント）

#### MCPサーバー異常終了時

1. 非ゼロ終了コード検出
2. 標準エラー出力取得
3. エラーログ記録
4. ジョブステータスを「failed」に更新
5. エラーメッセージをそのまま返却
6. メトリクス更新（エラーカウント）

#### ファイル生成失敗時

1. プロセスは正常終了したがファイルが存在しない
2. ディレクトリ内容確認
3. MCPレスポンス内容確認
4. エラーログ記録
5. ジョブステータスを「failed」に更新
6. MCPレスポンスをそのまま返却

#### 並行実行制限到達時

1. Semaphore 取得試行
2. 即座に取得できない場合
3. リトライせず429エラー返却
4. Retry-Afterヘッダー付与
5. クライアント側でリトライ推奨
6. メトリクス更新（制限到達カウント）

#### ディスク容量不足時

1. ファイル書き込み時にOSError検出
2. エラーログ記録
3. 部分的に作成されたファイル削除
4. ジョブディレクトリ削除
5. クライアントに507エラー返却
6. メトリクス更新（エラーカウント）

### 5.3 ファイルダウンロードフロー（Nginx経由）

#### ダウンロードリクエスト処理（Nginx）

1. ダウンロードURLにアクセス（GET /files/{job-uuid}/{filename}）
2. job-uuid抽出
3. ジョブディレクトリ存在確認
4. metadata.json読み込み（Nginxスクリプトまたはヘルパー）
5. 有効期限チェック
6. ファイルパス構築
7. ファイル存在確認
8. MIME type判定
9. Content-Dispositionヘッダー設定
10. ファイルストリーミング送信

#### 有効期限切れ時

1. expires_at と現在時刻を比較
2. 期限切れの場合404 Not Found返却
3. 後続のGC処理でファイル削除

## 6. API仕様設計

### 6.1 エンドポイント一覧

#### 6.1.1 MCPOエンドポイント（複数サーバータイプ対応）

各MCPサーバータイプごとに独立したエンドポイントを提供：

- パス：/mcpo/{server-type}
- 例：
  - /mcpo/pptx-generator
  - /mcpo/pdf-generator
  - /mcpo/excel-generator
- メソッド：POST
- Content-Type：application/json
- プロトコル：JSON-RPC 2.0

#### 6.1.2 MCPエンドポイント（複数サーバータイプ対応）

各MCPサーバータイプごとに独立したエンドポイントを提供：

- パス：/mcp/{server-type}
- 例：
  - /mcp/pptx-generator
  - /mcp/pdf-generator  
  - /mcp/excel-generator
- メソッド：POST
- Content-Type：application/json
- プロトコル：MCP標準プロトコル

#### 6.1.3 ヘルスチェックエンドポイント

- パス：/health
- メソッド：GET
- レスポンス：JSON

#### 6.1.4 メトリクスエンドポイント

- パス：/metrics
- メソッド：GET
- レスポンス：Prometheus形式

### 6.2 MCPOエンドポイント詳細

#### リクエスト形式

JSON-RPC 2.0形式のリクエストを受け付け、そのままMCPサーバーに転送：

- jsonrpc：バージョン文字列（固定値「2.0」）
- method：呼び出すツール名（MCPサーバーで定義されたツール）
- params：ツールパラメータオブジェクト（バリデーションなし）
- id：リクエストID（数値または文字列）

#### レスポンス形式

MCPサーバーからのレスポンスをそのまま返却：

- Bridge側でレスポンスの内容を変更しない
- MCPサーバーが生成したJSONをそのままクライアントに転送
- エラーレスポンスもMCPサーバーの形式をそのまま使用

### 6.3 MCPエンドポイント詳細

#### リクエスト形式

MCP標準プロトコルのリクエストを受け付け、そのままMCPサーバーに転送：

- MCP仕様に従ったリクエスト形式
- Bridgeではバリデーションを行わず、MCPサーバーに委譲

#### レスポンス形式

MCPサーバーからのレスポンスをそのまま返却：

- Bridge側でレスポンスの内容を変更しない
- MCPサーバーが生成したレスポンスをそのままクライアントに転送

### 6.4 ヘルスチェックエンドポイント

#### エンドポイント仕様

- メソッド：GET
- パス：/health
- レスポンス：JSON

#### レスポンス内容

- status：健全性ステータス（ok / degraded / down）
- timestamp：チェック実行時刻
- version：Bridgeバージョン
- uptime：稼働時間（秒）

#### 判定基準

- 正常：すべてのコンポーネントが動作中
- 劣化：一部リソースが制限に近い
- 停止：重大な問題発生

### 6.5 メトリクスエンドポイント

#### エンドポイント仕様

- メソッド：GET
- パス：/metrics
- レスポンス：Prometheus形式テキスト

#### 公開メトリクス

##### リクエスト関連

- mcpo_requests_total：総リクエスト数（server_type, status別）
- mcpo_request_duration_seconds：リクエスト処理時間（ヒストグラム）
- mcpo_requests_in_progress：現在処理中のリクエスト数

##### プロセス関連

- mcpo_processes_started_total：起動したMCPサーバープロセス総数
- mcpo_processes_failed_total：失敗したプロセス総数
- mcpo_process_duration_seconds：プロセス実行時間（ヒストグラム）

##### ジョブ関連

- mcpo_jobs_active：アクティブなジョブ数
- mcpo_jobs_completed_total：完了したジョブ総数
- mcpo_jobs_failed_total：失敗したジョブ総数

##### リソース関連

- mcpo_semaphore_available：利用可能なセマフォ数
- mcpo_disk_usage_bytes：ディスク使用量
- mcpo_files_count：管理中のファイル数

## 7. データ設計

### 7.1 メタデータ構造

#### metadata.json スキーマ

各ジョブディレクトリに配置されるメタデータファイルの構造：

- job_id（string）：ジョブ一意識別子
- server_name（string）：使用したMCPサーバー名
- created_at（string）：作成日時（ISO 8601）
- expires_at（string）：有効期限（ISO 8601）
- status（string）：ジョブ状態（processing / completed / failed）
- request（object）：受信したMCPリクエスト
- response（object）：MCPサーバーレスポンス（処理完了後）
- error（string）：エラーメッセージ（失敗時のみ）
- output_files（array）：生成ファイル情報配列
  - filename（string）：ファイル名
  - size（number）：サイズ（バイト）
  - mime_type（string）：MIMEタイプ

### 7.2 設定ファイル構造

#### mcp-servers.json スキーマ

MCPサーバー設定ファイルの構造：

- mcpServers（object）：サーバー定義マップ
  - {server-name}（object）：個別サーバー設定
    - command（string）：実行コマンドパス
    - args（array of string）：コマンドライン引数
    - env（object）：環境変数マップ（オプショナル）

#### 特殊トークン

args配列内で使用可能な特殊トークン：

- __WORKDIR__：ジョブ作業ディレクトリパスに置換される
- __JOB_ID__：ジョブUUIDに置換される

### 7.3 環境変数設計

#### Bridge設定用環境変数

- MCPO_CONFIG_FILE：MCP設定ファイルパス（デフォルト：/app/config/mcp-servers.json）
- MCPO_JOBS_DIR：ジョブディレクトリルート（デフォルト：/tmp/mcpo-jobs）
- MCPO_BASE_URL：ダウンロードURL用ベースURL（デフォルト：http://nginx）
- MCPO_FILE_EXPIRY：ファイル有効期限（秒）（デフォルト：3600）
- MCPO_MAX_CONCURRENT：最大同時実行数（デフォルト：CPU数×4）
- MCPO_TIMEOUT：プロセスタイムアウト（秒）（デフォルト：300）
- MCPO_LOG_LEVEL：ログレベル（デフォルト：INFO）

#### MCPサーバー実行時環境変数

MCPサーバープロセスに渡される環境変数：

- MCPO_WORKDIR：作業ディレクトリパス
- MCPO_JOB_ID：ジョブID
- その他、mcp-servers.jsonのenv設定

## 8. セキュリティ設計

### 8.1 基本方針

本システムでは、以下のセキュリティ方針を採用：

- 入力検証はすべてMCPサーバーに委譲
- Dockerコンテナはroot権限で実行
- ファイルシステムのパーミッション制御は最小限
- リソース制限は設定しない
- シンプルな設計を優先

### 8.2 実施するセキュリティ対策

#### パス検証

- ジョブディレクトリパス正規化
- 親ディレクトリトラバーサル防止
- シンボリックリンク検出
- ホワイトリストベースパス検証

#### ファイル名検証

- 許可文字制限（英数字、ハイフン、アンダースコア、ドット）
- 長さ制限（255バイト以内）

#### Docker セキュリティ

##### イメージセキュリティ

- 公式Pythonイメージをベースに使用
- 最小限のパッケージインストール
- 定期的なイメージ更新
- 脆弱性スキャン実施

##### ネットワークセキュリティ

- 内部ネットワーク分離
- 不要ポート非公開
- TLS通信（本番環境）

## 9. スケーラビリティ設計

### 9.1 垂直スケール（単一インスタンス）

#### CPU拡張

- マルチコアCPUによる並列処理
- asyncioによる非同期I/O活用
- Semaphoreによる並行数最適化

#### メモリ拡張

- ファイルストリーミング送信によるメモリ節約
- ジョブメタデータの軽量化
- 不要オブジェクトの早期解放

#### ストレージ拡張

- ボリュームサイズ拡張
- 古いファイルの積極的削除
- ストレージ使用量監視

### 9.2 水平スケール（複数インスタンス）

#### Docker Compose Replicas

- docker-compose.ymlでreplicas設定
- 同一設定の複数Bridgeインスタンス起動
- 自動的にNginxロードバランサー配下に配置

#### ステートレス設計

- インスタンス間で状態共有なし
- セッション情報なし
- ジョブIDによる一意性担保

#### Nginxロードバランシング

- ラウンドロビン分散
- ヘルスチェック連動
- セッションアフィニティ不要
- フェイルオーバー自動処理

#### 共有ストレージ

- 名前付きDocker Volumeで全インスタンス共有
- 全インスタンスが同一ストレージアクセス
- ダウンロードリクエストはどのインスタンスでも処理可能（Nginx経由）

### 9.3 性能目標

#### レスポンスタイム

- MCPリクエスト受信からレスポンス返却まで：平均5秒以内
- ファイルダウンロード開始まで：1秒以内

#### スループット

- 単一インスタンス：10リクエスト/秒
- 複数インスタンス：100リクエスト/秒以上

#### 同時接続数

- 単一インスタンス：50並行処理
- 複数インスタンス：500並行処理以上

## 10. ガーベジコレクション設計

### 10.1 削除対象の判定

#### 期限切れジョブ

- metadata.jsonのexpires_at参照
- 現在時刻と比較
- 期限切れジョブを削除対象としてマーク

#### 孤児ディレクトリ

- metadata.jsonが存在しないディレクトリ
- 作成から一定時間経過（デフォルト24時間）
- 異常終了等で残存したディレクトリ

### 10.2 削除タイミング

#### 定期実行（cron方式）

- 実行間隔：5分毎（デフォルト）
- バックグラウンドタスクとして実行
- asyncioスケジューラー利用

#### 起動時クリーンアップ

- Bridge起動時に1回実行
- 前回異常終了時の残存ファイル削除
- 整合性確保

### 10.3 削除処理フロー

#### Phase 1: スキャン

1. /tmp/mcpo-jobs ディレクトリ走査
2. 各ジョブディレクトリのmetadata.json読み込み
3. expires_at確認
4. 削除対象リスト作成

#### Phase 2: 削除実行

1. 削除対象ディレクトリに対して順次処理
2. ディレクトリ内全ファイル削除
3. ディレクトリ自体を削除
4. 削除成功ログ記録

#### Phase 3: エラーハンドリング

1. 削除失敗時はログ記録
2. 次回GC実行時にリトライ
3. 連続失敗時はアラート（将来実装）

### 10.4 安全性担保

#### パス検証

- 削除対象が/tmp/mcpo-jobs配下か厳密確認
- 親ディレクトリ削除防止
- シンボリックリンク追跡禁止

#### ログ記録

- 削除ジョブID記録
- 削除日時記録
- エラー詳細記録

## 11. エラーハンドリング設計

### 11.1 エラー分類

#### レベル1：リクエストエラー

- クライアント起因のエラー
- 4xxステータスコード
- リトライ不要

#### レベル2：処理エラー

- MCPサーバー起因のエラー
- 5xxステータスコード
- リトライ可能な場合あり

#### レベル3：システムエラー

- Bridge自体の障害
- 503ステータスコード
- リトライ推奨

### 11.2 エラーレスポンス設計

#### 基本方針

MCPサーバーからのエラーレスポンスをそのまま返却：

- Bridgeでエラーメッセージを加工しない
- MCPサーバーのエラー形式を保持
- トレース情報もMCPサーバーの出力をそのまま使用

### 11.3 ログ設計

#### ログレベル

- DEBUG：デバッグ情報
- INFO：通常動作ログ
- WARNING：警告（処理は継続）
- ERROR：エラー（処理失敗）
- CRITICAL：致命的エラー（サービス停止）

#### ログ出力先

- 標準出力（Dockerログに集約）
- ログボリュームに永続化（必須）
- 本番環境では外部ログ集約サービス連携

#### ログフォーマット

- JSON形式
- タイムスタンプ、ログレベル、メッセージ、コンテキスト情報含む
- トレースIDによるリクエスト追跡

### 11.4 監視・アラート設計

#### 監視項目

- エラー発生率
- レスポンスタイム
- 同時実行数
- ディスク使用率
- プロセス起動失敗率

#### アラート条件

- エラー率閾値超過
- レスポンスタイム劣化
- ディスク容量逼迫
- 連続処理失敗

## 12. Docker化設計

### 12.1 Dockerfile設計方針

#### ベースイメージ

- 公式Pythonイメージ使用（python:3.11）
- セキュリティアップデート適用済みイメージ選定
- root権限での実行

#### マルチステージビルド

- ビルドステージ：依存関係インストール
- 実行ステージ：最小限のファイルのみコピー
- イメージサイズ削減

#### レイヤー最適化

- 変更頻度の低いコマンドを先に配置
- キャッシュ効率最大化
- 不要ファイル除外

### 12.2 ディレクトリ構造

コンテナ内ディレクトリ配置：

```
/app/
  ├── main.py                 # アプリケーションエントリーポイント
  ├── requirements.txt        # Python依存パッケージ
  ├── config/                 # 設定ファイルディレクトリ（ソースコード配下）
  │   ├── mcp-servers.json    # MCPサーバー定義
  │   └── mcp-servers.json.example  # サンプル設定
  └── src/                    # ソースコードディレクトリ
      ├── api/                # APIエンドポイント
      ├── core/               # コアロジック
      ├── models/             # データモデル
      └── utils/              # ユーティリティ

/tmp/mcpo-jobs/             # 一時ファイル作業領域（Volume）

/var/log/mcpo/              # ログディレクトリ（Volume）
```

### 12.3 依存パッケージ管理

#### Python依存パッケージ

requirements.txtで管理する主要パッケージ：

- FastAPI：Webフレームワーク
- Uvicorn：ASGIサーバー
- Pydantic：データバリデーション
- aiofiles：非同期ファイルI/O
- python-multipart：ファイルアップロード対応
- prometheus-client：Prometheusメトリクス収集

#### システムパッケージ

実行時に必要なパッケージ：

- curl：ヘルスチェック用
- ca-certificates：HTTPS通信用

### 12.4 ボリューム設計

#### 一時ファイルボリューム

- ボリューム名：mcpo-jobs
- マウントポイント：/tmp/mcpo-jobs
- 永続化ストレージ
- 全Bridgeインスタンスで共有

#### ログボリューム（必須）

- ボリューム名：mcpo-logs
- マウントポイント：/var/log/mcpo
- 永続化ログ保存
- 全Bridgeインスタンスで共有

### 12.5 ネットワーク設計

#### 内部ネットワーク

- ネットワーク名：mcpo-network
- ドライバー：bridge
- Nginx、OpenWebUI、Bridgeが接続

#### ポート公開

- Nginxポート：80（HTTP）、443（HTTPS、本番環境）
- Bridgeポート：8080（内部のみ、Nginx経由）
- OpenWebUIポート：内部のみ（Nginx経由）

### 12.6 環境変数設計

#### 必須環境変数

- MCPO_BASE_URL：ダウンロードURL生成用
- 例：http://nginx

#### オプション環境変数

- MCPO_FILE_EXPIRY：ファイル有効期限
- MCPO_MAX_CONCURRENT：最大同時実行数
- MCPO_TIMEOUT：タイムアウト時間
- MCPO_LOG_LEVEL：ログレベル

### 12.7 ヘルスチェック設計

#### Dockerヘルスチェック

- チェック方法：HTTP GET /health
- 間隔：30秒
- タイムアウト：10秒
- リトライ：3回
- 開始遅延：10秒

#### ヘルスチェック判定

- 成功：200 OK応答
- 失敗：タイムアウトまたは非200応答
- unhealthy判定後の動作：Docker依存

## 13. Docker Compose設計

### 13.1 サービス定義

#### Nginxサービス

- サービス名：nginx
- イメージ：公式nginxイメージ
- ポート：ホスト80 → コンテナ80
- ボリューム：nginx設定、mcpo-jobs（読み取り専用）
- ネットワーク：mcpo-network
- 設定：ロードバランシング、静的ファイル配信

#### OpenWebUIサービス

- サービス名：openwebui
- イメージ：公式OpenWebUIイメージ
- ポート：内部のみ（Nginx経由）
- ボリューム：openwebui-data（永続化）
- ネットワーク：mcpo-network
- 依存関係：mcpo-bridge（depends_on）

#### MCPO Bridgeサービス

- サービス名：mcpo-bridge
- ビルド：ローカルDockerfile使用
- ポート：内部8080（Nginx経由のみ）
- ボリューム：mcpo-jobs、mcpo-logs
- ネットワーク：mcpo-network
- ヘルスチェック：有効
- デプロイ：replicas設定で複数インスタンス

### 13.2 ボリューム定義

#### 名前付きボリューム

- openwebui-data：OpenWebUIデータ永続化
- mcpo-jobs：Bridge一時ファイル（全インスタンス共有）
- mcpo-logs：Bridgeログファイル（必須、全インスタンス共有）

### 13.3 ネットワーク定義

#### mcpo-network

- ドライバー：bridge
- 内部通信専用
- DNSによるサービス名解決

### 13.4 スケール設定

#### Replicas設定

- mcpo-bridgeサービスにdeploy.replicas設定
- デフォルト：3インスタンス
- Nginxが自動的にロードバランシング

### 13.5 起動順序制御

#### depends_on設定

- OpenWebUIはmcpo-bridgeに依存
- mcpo-bridgeのhealthy確認後にOpenWebUI起動
- condition: service_healthy 利用

#### 起動シーケンス

1. ネットワーク作成
2. ボリューム作成
3. mcpo-bridge起動（複数レプリカ）
4. mcpo-bridgeヘルスチェック待機
5. nginx起動
6. openwebui起動
7. システム利用可能

### 13.6 再起動ポリシー

#### unless-stoppedポリシー

採用理由：
- 異常終了時の自動復旧
- ホスト再起動時の自動起動
- 手動停止時は再起動しない

## 14. MCP設定ファイル詳細仕様

### 14.1 設定ファイル配置

#### ファイルパス

- ソースコード配下：/app/config/mcp-servers.json
- サンプルファイル：/app/config/mcp-servers.json.example
- 環境変数で変更可能：MCPO_CONFIG_FILE

#### ファイル形式

- エンコーディング：UTF-8
- フォーマット：JSON
- スキーマバリデーション：起動時実施

### 14.2 設定ファイル構造詳細

#### ルートオブジェクト

- mcpServers：サーバー定義マップ（必須）

#### サーバー定義オブジェクト

各サーバーは以下のフィールドを持つ：

- command：実行バイナリパス（必須）
  - 絶対パスまたは相対パス
  - PATH環境変数で解決可能なコマンド名
- args：コマンドライン引数配列（必須、空配列可）
  - 文字列の配列
  - 特殊トークン置換対応
- env：環境変数マップ（オプション）
  - キーバリューペアのオブジェクト
  - サーバー実行時に設定
- timeout：タイムアウト秒数（オプション）
  - 個別サーバーのタイムアウト設定
  - グローバル設定を上書き

### 14.3 特殊トークン仕様

#### __WORKDIR__

- 用途：作業ディレクトリパス指定
- 置換内容：/tmp/mcpo-jobs/{job-uuid}
- 使用例：args配列内に"--output"、"__WORKDIR__"と記述

#### __JOB_ID__

- 用途：ジョブID指定
- 置換内容：job-uuid文字列
- 使用例：args配列内に"--job-id"、"__JOB_ID__"と記述

#### 置換タイミング

- MCPサーバープロセス起動直前
- args配列を走査して全トークンを置換
- 環境変数envには自動設定（MCPO_WORKDIR等）

### 14.4 設定リロード

#### 現行仕様

- 設定ファイル変更時はBridge再起動が必要
- 動的リロード機能なし

## 15. Nginx設定設計

### 15.1 ロードバランサー設定

#### アップストリーム定義

- mcpo-bridgeサービスの全レプリカを自動検出
- Docker DNSによる名前解決
- ラウンドロビンアルゴリズム

#### ヘルスチェック

- 各Bridgeインスタンスの/healthエンドポイントを監視
- unhealthyなインスタンスを自動除外

### 15.2 静的ファイル配信設定

#### ロケーション設定

- パス：/files/
- ルート：共有ボリューム /tmp/mcpo-jobs
- ディレクトリリスティング：無効

#### 有効期限チェック

- Nginxスクリプトまたは外部ヘルパーでmetadata.json参照
- expires_at超過時は404返却

### 15.3 プロキシ設定

#### MCPOエンドポイント

- パス：/mcpo/
- プロキシ先：mcpo-bridgeアップストリーム
- タイムアウト：600秒（長時間処理対応）

#### MCPエンドポイント

- パス：/mcp/
- プロキシ先：mcpo-bridgeアップストリーム
- タイムアウト：600秒

### 15.4 メトリクス集約

#### 集約エンドポイント

- パス：/metrics
- 処理：各Bridgeインスタンスの/metricsを収集・結合

## 16. 運用設計

### 16.1 デプロイ手順

#### 初回デプロイ

1. リポジトリクローン
2. config/mcp-servers.json.example を config/mcp-servers.json にコピーして編集
3. docker-compose.yml 環境変数調整
4. docker-compose up -d 実行
5. ヘルスチェック確認
6. OpenWebUI アクセス確認

#### 更新デプロイ

1. 最新コードpull
2. docker-compose build 実行
3. docker-compose up -d 実行
4. ヘルスチェック確認
5. 動作確認

### 16.2 監視項目

#### サービス監視

- コンテナ稼働状態
- ヘルスチェック結果
- リスタート回数

#### リソース監視

- CPU使用率
- メモリ使用量
- ディスク使用量（ボリューム）
- ネットワークトラフィック

#### アプリケーション監視

- リクエスト数
- エラー率
- レスポンスタイム
- 同時実行プロセス数

#### Prometheusメトリクス

- /metricsエンドポイントからメトリクス取得
- Grafanaダッシュボードで可視化
- アラート設定

### 16.3 バックアップ

#### 対象データ

- config/mcp-servers.json 設定ファイル
- docker-compose.yml
- Nginx設定ファイル

#### バックアップ頻度

- 設定ファイル：変更時
- ログ：定期的にアーカイブ
- 一時ファイル：不要（自動削除前提）

### 16.4 障害対応

#### コンテナ再起動

- docker-compose restart mcpo-bridge
- 自動再起動ポリシーにより自動復旧も可能

#### ログ確認

- docker-compose logs mcpo-bridge
- ログボリュームから直接確認
- エラー原因特定

#### ロールバック

- 前バージョンイメージへの切り戻し
- docker-compose down → イメージタグ変更 → up

### 16.5 スケールアウト

#### 手動スケール

- docker-compose.ymlのreplicas設定変更
- docker-compose up -d で反映
- Nginxが自動的に新インスタンスを検出

## 17. テスト設計

### 17.1 ユニットテスト

#### テスト対象

- ジョブ管理関数
- メタデータ生成関数
- トークン置換関数

#### テストフレームワーク

- pytest
- モックライブラリ使用

### 17.2 統合テスト

#### テストシナリオ

- エンドツーエンドMCP/MCPOリクエスト処理
- ファイルダウンロード（Nginx経由）
- エラーハンドリング
- タイムアウト処理

#### テスト環境

- Docker Compose によるテスト環境構築
- モックMCPサーバー使用

### 17.3 負荷テスト

#### テストツール

- Apache Bench (ab)
- Locust
- k6

#### テスト項目

- 同時リクエスト数増加
- レスポンスタイム測定
- エラー率測定
- スケーリング効果確認

## 18. まとめ

本詳細設計書は、MCPO On-Demand MCP Bridgeシステムの実装に必要な全ての設計情報を網羅している。

### 主要な設計ポイント

- Nginxフロントエンドによる効率的なファイル配信とロードバランシング
- MCP/MCPOエンドポイントの明確な分離と複数サーバータイプ対応
- MCPレスポンスの透過的な転送（変更なし）
- 入力検証のMCPサーバーへの委譲
- Docker Compose replicasによる簡単なスケールアウト
- Prometheusメトリクスによる運用監視
- シンプルで保守しやすい設計

本設計に基づき実装を進めることで、安全でスケーラブルなファイル生成系MCPサーバー基盤を構築できる。

---

**MCPO On-Demand Bridge 詳細設計書 v2.0**
